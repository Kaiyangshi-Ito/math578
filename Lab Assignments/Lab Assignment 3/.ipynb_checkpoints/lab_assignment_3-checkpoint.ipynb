{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH 578 Lab Assignment 3 <a class=\"tocSkip\">\n",
    "\n",
    "\n",
    "***Kai Yang***\n",
    "\n",
    "Original assignment problems are put in *italic fonts*, my text answers are put in **bold fonts**. For both parts, the functions written are in the constructed classes. \n",
    "\n",
    "Another interesting thing to note is that, to run the entire thing on GPU, simply use `cupy` instead of `numpy` -- but for the purpose of the assignment, we will stay with `numpy`. The installed `numpy` version at time of completion of this assignment is `1.19.2`.\n",
    "\n",
    "**All computational functions are consisted in the following class `lab3`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T02:50:29.859144Z",
     "start_time": "2020-12-07T02:50:29.398496Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import timeit\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class lab3:\n",
    "    '''\n",
    "    The class created for this lab assignment. Basically a class to implement Runge-Kutta to numerically solve first order continuous time IVP. \n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Class constructor \n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    def Euler(self, direction, h, num_iter, f, u_0, t_0, tol=1e-6, nabla_f=\"empty\"): \n",
    "        '''\n",
    "        (for problem 1) Forward/backward Euler's method\n",
    "        Parameters: \n",
    "            direction: string\n",
    "                Choose \"forward\" for forward Euler's method, or \"backward\" for backward Euler's method \n",
    "            h: floating point scalar\n",
    "                Step size for Euler's method \n",
    "            num_iter: integer scalar\n",
    "                Number of iterations to run\n",
    "            f: function\n",
    "                Function for du/dt; the function itself should take value of 1-D array u and scalar t, and output 1-D array as values of f\n",
    "            u_0: floating point 1-D array\n",
    "                Initial value for u \n",
    "            t_0: floating point scalar\n",
    "                Initial value for t \n",
    "            tol: floating point scalar; default 1e-6\n",
    "                Tolerance parameter to pass to self.newton\n",
    "            nabla_f: function; default \"empty\" for it's not used for forward Euler's method \n",
    "                Function for derivative of f; the function itself should take value of 1-D array u and scalar t, and output 2-D array as the derivative of f\n",
    "        Returns:\n",
    "            u: floating point 2-D array\n",
    "                Array consisting of u values corresponding to t values above\n",
    "            t: floating point 1-D array\n",
    "                Array consisting of t values\n",
    "            N: interger scalar\n",
    "                Total number of function evaluations\n",
    "        '''\n",
    "        # initialize arrays used for outputs \n",
    "        u = np.empty((num_iter+1, len(u_0)))\n",
    "        u[0] = u_0 \n",
    "        t = np.arange(num_iter+1, dtype=float)\n",
    "        t *= h\n",
    "        t += t_0\n",
    "        N=0\n",
    "        \n",
    "        # forward Euler's method\n",
    "        if direction == \"forward\":\n",
    "            for k in np.arange(num_iter): \n",
    "                u[k+1,:] = u[k,:] + h*f(u=u[k,:], t=t[k])\n",
    "                N += 1\n",
    "        # backward Euler's method\n",
    "        elif direction == \"backward\":\n",
    "            if nabla_f == \"empty\":\n",
    "                return \"Please input nabla_f; i.e., the function for derivative of f.\" \n",
    "            for k in np.arange(num_iter):\n",
    "                x = u[k,:] + h*f(u=u[k,:], t=t[k]) # use forward Euler to make an initial guess\n",
    "                # Newton iteration \n",
    "                converged = False\n",
    "                while not converged:\n",
    "                    g = x - u[k,:] - h*f(u=x, t=t[k+1])\n",
    "                    N += 1\n",
    "                    nabla_g = np.eye(len(u_0)) - h*nabla_f(x, t)\n",
    "                    d = np.linalg.solve(nabla_g, g) # here I use numpy built-in method to solve linear system -- it probably uses a modified GMRES or CG anyway, not the point for this assignemnt \n",
    "                    converged = np.allclose(0, d, rtol=0, atol=tol, equal_nan=False) # the updating formula for Newton's iteration allows us to do this \n",
    "                    x -= d\n",
    "                u[k+1,:] = x\n",
    "        # in case there is a mistake in input... \n",
    "        else:\n",
    "            print(\"Choose \\\"forward\\\" for forward Euler's method, or \\\"backward\\\" for backward Euler's method.\")\n",
    "        return u, t, N\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 \n",
    "*Implement forward and backward Euler methods (for vector valued functions), where the backward Euler code uses Newton's iteration as a nonlinear solver. In each step of the backward Euler, start the Newton iteration with the initial guess given by a step of the forward Euler method, and stop the Newton iteration when the distance between two successive iterates falls within some specified tolerance (e.g., equal to $10^{-6}$ ).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 \n",
    "*Use both the forward and backward Euler methods on the three model problems $y^\\prime= \\lambda y$, $y(0) = 1$ where (i) $\\lambda = -23$, (ii) $\\lambda = 1$ and also (iii) $$y^{\\prime}=\\left(\\begin{array}{cc}\n",
    "-1 & 0\\\\\n",
    "0 & -100\n",
    "\\end{array}\\right)y,\\ y\\left(0\\right)=\\left(\\begin{array}{c}\n",
    "1\\\\\n",
    "1\n",
    "\\end{array}\\right).$$ using time steps $h = 0.1, 0.05, 0.02, 0.01, 0.005,\\dots$. For each method for each program:*\n",
    "\n",
    "*(a). Plot a graph of $\\log\\left\\Vert y(t_{n})-y_{n}\\right\\Vert$ against $\\log h$, at the fixed time $t=2$ where $n$ satisfies $n h=2$, that is we are computing many numerical solutions, each for a different fixed step-size $h$, for $t \\in \\left[0, 2\\right]$ and comparing the error between the exact and numerical solution at the final time as a function of $h$. Explain the results.*\n",
    "\n",
    "*(b). To compare the performance of the methods more fairly, we should consider the extra work required when computing with the backward Euler method. So, repeat the first part, but this time plot $\\log\\left\\Vert y(t_{n})-y_{n}\\right\\Vert$ against $N$ where $N$ is the total number of function evaluations in the integration. For the forward Euler method $N = n$, but for the backward Euler method it will be the total number of Newton iteration in the first $n$ steps. Which method is most efficient for $h$ small? Is there a difference for $h$ large?*\n",
    "\n",
    "*(c). For one fixed $h$ plot graphs of $\\log\\left\\Vert y(t_{n})-y_{n}\\right\\Vert$ against $t_n$. Ensure that the interval of time integration is sufficiently long to see the qualitative difference in the behaviour between cases (i) and (ii). Explain this difference. Which method performs best? Does changing $h$ change the relative differences between the methods? Plot the over-imposed graphs of the computed solutions and the exact solution (for case (iii) one can separate the components of $y$ to get 2-D plots).*\n",
    "\n",
    "### (a).\n",
    "**For (i) and (ii), the solution for this ODE is $$y=e^{\\lambda t}$$ so for (i), $y_n=e^{-46}$; and for (ii), $y_n=e^2$. For (iii), thanks to the diagonal matrix, similarly we can calculate $y_{n}=\\left[\\begin{array}{c}\n",
    "e^{-2}\\\\\n",
    "e^{-200}\n",
    "\\end{array}\\right]$.**\n",
    "**In the plot, it appears that when $h$ is large, the difference between forward and backward Euler's method is the most outstanding; as $h$ becomes smaller, the difference tends to disappear -- at least for (i) and (iii). Explain in optimization terms, forward Euler's method is analogous to gradient descent; while backward Euler's method is analogous to Newton-Ralphson here; because (ii) has very little curvature info to be incorporated, Newton-Ralphson won't outperform gradient descent much -- not notable at all. As for the trend that the difference disappear: over a fixed interval $\\left[0,2\\right]$, more \"interpolation\" points results in the numerical values obtained, $u_n$, becomes closer to real values, $y_n$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-07T02:50:29.443Z"
    }
   },
   "outputs": [],
   "source": [
    "lab = lab3()\n",
    "def f_lambda_i(u, t):\n",
    "    return -23*u\n",
    "def nabla_f_lambda_i(u, t):\n",
    "    return -23\n",
    "def f_lambda_ii(u, t):\n",
    "    return u\n",
    "def nabla_f_lambda_ii(u, t):\n",
    "    return 1\n",
    "\n",
    "A = np.diag([-1,-100])\n",
    "def f_lambda_iii(u, t):\n",
    "    return A@u\n",
    "def nabla_f_lambda_iii(u, t):\n",
    "    return A\n",
    "\n",
    "h_vals = np.array([.1, .05, .02])\n",
    "h_vals_exp = 10**(-np.arange(0,4.))\n",
    "h_vals_exp = h_vals_exp.reshape(-1,1)\n",
    "h_vals = h_vals*h_vals_exp\n",
    "h_vals = h_vals.reshape(-1)\n",
    "\n",
    "# (i). \n",
    "FE_error = []\n",
    "BE_error = []\n",
    "FE_N = []\n",
    "BE_N = []\n",
    "\n",
    "for h in h_vals:\n",
    "    u_FE_temp, t_FE_temp, FE_N_temp = lab.Euler(direction = \"forward\", h=h, num_iter=np.int(2/h), f=f_lambda_i, u_0=np.array([1.]), t_0=0.)\n",
    "    FE_error += [u_FE_temp[-1,0]]\n",
    "    FE_N += [FE_N_temp]\n",
    "    u_BE_temp, t_BE_temp, BE_N_temp = lab.Euler(direction = \"backward\", h=h, num_iter=np.int(2/h), f=f_lambda_i, u_0=np.array([1.]), t_0=0., nabla_f=nabla_f_lambda_i)\n",
    "    BE_error += [u_BE_temp[-1,0]]\n",
    "    BE_N += [BE_N_temp]\n",
    "\n",
    "FE_error = np.array(FE_error) - np.exp(-46)\n",
    "BE_error = np.array(BE_error) - np.exp(-46)\n",
    "FE_error_i = np.log(np.abs(FE_error))\n",
    "BE_error_i = np.log(np.abs(BE_error))\n",
    "FE_N_i = np.array(FE_N)\n",
    "BE_N_i = np.array(BE_N)\n",
    "\n",
    "\n",
    "# (ii). \n",
    "FE_error = []\n",
    "BE_error = []\n",
    "FE_N = []\n",
    "BE_N = []\n",
    "\n",
    "for h in h_vals:\n",
    "    u_FE_temp, t_FE_temp, FE_N_temp  = lab.Euler(direction = \"forward\", h=h, num_iter=np.int(2/h), f=f_lambda_ii, u_0=np.array([1.]), t_0=0.)\n",
    "    FE_error += [u_FE_temp[-1,0]]\n",
    "    FE_N += [FE_N_temp]\n",
    "    u_BE_temp, t_BE_temp, BE_N_temp = lab.Euler(direction = \"backward\", h=h, num_iter=np.int(2/h), f=f_lambda_ii, u_0=np.array([1.]), t_0=0., nabla_f=nabla_f_lambda_ii)\n",
    "    BE_error += [u_BE_temp[-1,0]]\n",
    "    BE_N += [BE_N_temp]\n",
    "\n",
    "FE_error = np.array(FE_error) - np.exp(2)\n",
    "BE_error = np.array(BE_error) - np.exp(2)\n",
    "FE_error_ii = np.log(np.abs(FE_error))\n",
    "BE_error_ii = np.log(np.abs(BE_error))\n",
    "FE_N_ii = np.array(FE_N)\n",
    "BE_N_ii = np.array(BE_N)\n",
    "\n",
    "\n",
    "# (iii). \n",
    "FE_error = []\n",
    "BE_error = []\n",
    "FE_N = []\n",
    "BE_N = []\n",
    "\n",
    "for h in h_vals:\n",
    "    u_FE_temp, t_FE_temp, FE_N_temp  = lab.Euler(direction = \"forward\", h=h, num_iter=np.int(2/h), f=f_lambda_iii, u_0=np.ones(2), t_0=0.)\n",
    "    FE_error += [np.linalg.norm(u_FE_temp[-1,:]-np.array([np.exp(-2),np.exp(-200)]))]\n",
    "    FE_N += [FE_N_temp]\n",
    "    u_BE_temp, t_BE_temp, BE_N_temp = lab.Euler(direction = \"backward\", h=h, num_iter=np.int(2/h), f=f_lambda_iii, u_0=np.ones(2), t_0=0., nabla_f=nabla_f_lambda_iii)\n",
    "    BE_error += [np.linalg.norm(u_BE_temp[-1,:]-np.array([np.exp(-2),np.exp(-200)]))]\n",
    "    BE_N += [BE_N_temp]\n",
    "\n",
    "FE_error_iii = np.log(np.abs(FE_error))\n",
    "BE_error_iii = np.log(np.abs(BE_error))\n",
    "FE_N_iii = np.array(FE_N)\n",
    "BE_N_iii = np.array(BE_N)\n",
    "\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(3, 1)\n",
    "axs[0].plot(np.log(h_vals), FE_error_i, '--o', label=\"Forward-Euler\")\n",
    "axs[0].plot(np.log(h_vals), BE_error_i, '--o', label=\"Backward-Euler\")\n",
    "axs[0].set(ylabel=r\"(i). $\\log(error)$\")\n",
    "axs[0].legend() # add a legend\n",
    "\n",
    "axs[1].plot(np.log(h_vals), FE_error_ii, '--o', label=\"Forward-Euler\")\n",
    "axs[1].plot(np.log(h_vals), BE_error_ii, '--o', label=\"Backward-Euler\")\n",
    "axs[1].set(ylabel=r\"(ii). $\\log(error)$\")\n",
    "axs[1].legend() # add a legend\n",
    "\n",
    "axs[2].plot(np.log(h_vals), FE_error_iii, '--o', label=\"Forward-Euler\")\n",
    "axs[2].plot(np.log(h_vals), BE_error_iii, '--o', label=\"Backward-Euler\")\n",
    "axs[2].set(ylabel=r\"(iii). $\\log(error)$\")\n",
    "axs[2].legend() # add a legend\n",
    "axs[2].set(xlabel=r\"$\\log(h)$\", ylabel=r\"$\\log(error)$\")\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b). \n",
    "\n",
    "**It seems that when comparing $N$, the efficiency for both methods are very close.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-07T02:50:29.465Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(3, 1)\n",
    "axs[0].plot(FE_N_i, FE_error_i, '--', label=\"Forward-Euler\")\n",
    "axs[0].plot(BE_N_i, BE_error_i, '--', label=\"Backward-Euler\")\n",
    "axs[0].set(ylabel=r\"(i). $\\log(error)$\", xlim=(0,4e4))\n",
    "axs[0].legend() # add a legend\n",
    "\n",
    "axs[1].plot(FE_N_ii, FE_error_ii, '--', label=\"Forward-Euler\")\n",
    "axs[1].plot(BE_N_ii, BE_error_ii, '--', label=\"Backward-Euler\")\n",
    "axs[1].set(ylabel=r\"(ii). $\\log(error)$\", xlim=(0,4e4))\n",
    "axs[1].legend() # add a legend\n",
    "\n",
    "axs[2].plot(FE_N_iii, FE_error_iii, '--', label=\"Forward-Euler\")\n",
    "axs[2].plot(BE_N_iii, BE_error_iii, '--', label=\"Backward-Euler\")\n",
    "axs[2].set(ylabel=r\"(iii). $\\log(error)$\", xlim=(0,4e4))\n",
    "axs[2].legend() # add a legend\n",
    "axs[2].set(xlabel=r\"$N$\", ylabel=r\"$\\log(error)$\")\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c). \n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 \n",
    "*Using the forward Euler code as a template, implement Runge's second order method $$\\begin{array}{c|cc}\n",
    "0\\\\\n",
    "\\frac{1}{2} & \\frac{1}{2}\\\\\n",
    "\\hline  & 0 & 1\n",
    "\\end{array}$$ and the four stage method (sometimes called \"The Runge-Kutta method\"): $$\\begin{array}{c|cccc}\n",
    "0\\\\\n",
    "\\frac{1}{2} & \\frac{1}{2}\\\\\n",
    "\\frac{1}{2} & 0 & \\frac{1}{2}\\\\\n",
    "1 & 0 & 0 & 1\\\\\n",
    "\\hline  & \\frac{1}{6} & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{6}\n",
    "\\end{array}$$ Compare the performance of these methods with the forward and backward Euler methods on the three differential equations considered in the previous exercise (use the same comparisons as in (a), (b) and (c) from that exercise).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 \n",
    "*In this exercise, we will study the motion of the five outer planets relative to the sun using different numerical integrators. The necessary data is given in file `outer.txt`. In the data file the units are chosen so that the sun has mass 1, the distances are in astronomical units (1a.u.$=149,597,870$km), and times in earth days. The sun and the inner planets are to be collectively treated as one object. When you copy and run `outer.txt` in Python, the data will be loaded to three variables `g`, `m`, and `x`; where `g` holds the value of the universal gravitational constant, `m` is an array consisting of the masses of the $6$ objects, and `x` is an array with initial positions and velocities of the objects, corresponding to September 5, 1994 at UTC 0:00. Further comments can be found in the file `outer.txt`.* \n",
    "\n",
    "*Implement the following numerical integrators and use them to study the gravitational 6 body problem with the data given in `outer.txt`. For each of the following cases, run the simulation for at least one to few thousand years (in simulated time of course), and describe what happens to the planets during the simulations. Adjust the stepsize of the method and the simulation time accordingly. Comment on the performance of the methods.* \n",
    "\n",
    "*(a) Explicit Euler with stepsize $10$ days*\n",
    "\n",
    "*(b) Implicit Euler with stepsize $10$ days*\n",
    "\n",
    "*(c) Your favorite Runge-Kutta method of order at least $2$, with stepsize $30$ days*\n",
    "\n",
    "*(d) Symplectic Euler with stepsize $100$ days*\n",
    "\n",
    "*(e) Stormer-Verlet with stepsize $200$ days*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-07T02:50:29.523Z"
    }
   },
   "outputs": [],
   "source": [
    "# open the data file for this problem \n",
    "exec(open('outer.txt').read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "author": "mes",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
